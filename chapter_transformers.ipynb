{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6562a98-4604-4e22-86e7-f8c9bb2171a8",
   "metadata": {},
   "source": [
    "# A new Chapter about Transformers"
   ]
  },
  {
   "cell_type": "code",
   "id": "b7bda8bb-757c-4036-a2b5-c37c7c1acb57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:05:24.803191Z",
     "start_time": "2025-03-30T09:05:22.103870Z"
    }
   },
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "ea61a202-1ae3-43d6-b4e8-53bbed60f798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:05:30.659429Z",
     "start_time": "2025-03-30T09:05:30.569503Z"
    }
   },
   "source": [
    "classifier(\n",
    "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so ≠≠≠much!\"]\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124},\n",
       " {'label': 'NEGATIVE', 'score': 0.9996541738510132}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "6b661e41-def6-4366-aade-59aea429a1a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:05:32.119354Z",
     "start_time": "2025-03-30T09:05:32.117212Z"
    }
   },
   "source": [
    "print('this is a new line!')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a new line!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "02e06945-6f3f-4b92-9b95-6e28c4dfc0a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:05:40.648971Z",
     "start_time": "2025-03-30T09:05:33.463389Z"
    }
   },
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445958495140076, 0.11197652667760849, 0.04342763125896454]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "4f6bc782-037a-4ada-be46-083bdbfe9007",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:05:40.743041Z",
     "start_time": "2025-03-30T09:05:40.654741Z"
    }
   },
   "source": [
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"a\", \"b\", \"c\"],\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['a', 'c', 'b'],\n",
       " 'scores': [0.6285725235939026, 0.1918037235736847, 0.17962372303009033]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "6b637c20-cb78-49e7-bc91-eb9423a8d520",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:05:41.568358Z",
     "start_time": "2025-03-30T09:05:40.748762Z"
    }
   },
   "source": [
    "\n",
    "generator = pipeline(\"text-generation\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "6d2f10b3-f418-4ae2-91b2-c755a1dec73c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:05:51.286167Z",
     "start_time": "2025-03-30T09:05:41.574681Z"
    }
   },
   "source": [
    "generator(\"In this course, we will teach you how to\", num_return_sequences=5, max_length=20)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to develop a web application like this. All you need'},\n",
       " {'generated_text': 'In this course, we will teach you how to build the simplest computer in the most cost effective way'},\n",
       " {'generated_text': 'In this course, we will teach you how to make an awesome game.\"\\n\\nOur goal is'},\n",
       " {'generated_text': 'In this course, we will teach you how to make a quick, easy-to-find wallet'},\n",
       " {'generated_text': 'In this course, we will teach you how to write a simple PHP script, use PHP to handle'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "0248373d-660d-4288-b644-2c70ee23cfe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:05:55.786815Z",
     "start_time": "2025-03-30T09:05:51.295346Z"
    }
   },
   "source": [
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to avoid the potential of having some of our own data exposed, in your own data. In our course'},\n",
       " {'generated_text': 'In this course, we will teach you how to identify the types of types of people you should target to be in control of:\\n\\n\\n\\n'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "5d7c7707-d7c3-4f76-a76a-8eafd68ec5da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:05:58.830506Z",
     "start_time": "2025-03-30T09:05:55.795536Z"
    }
   },
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19198693335056305,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models.'},\n",
       " {'score': 0.04209252446889877,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This course will teach you all about computational models.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "6fca2ca4-8273-417e-bb29-3bd4f7132d34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:02.156404Z",
     "start_time": "2025-03-30T09:05:58.839192Z"
    }
   },
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n",
      "/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.9981694),\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.97960186),\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9932106),\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "568a37f9-ef33-469a-89be-209f974c48bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:03.128443Z",
     "start_time": "2025-03-30T09:06:02.167892Z"
    }
   },
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6949760913848877, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "d3e0ac51-9bcc-4724-8b63-0007cdb2192d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:14.840289Z",
     "start_time": "2025-03-30T09:06:03.146943Z"
    }
   },
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "c286f392-c741-4fb0-9459-00e4637cb64f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:20.412521Z",
     "start_time": "2025-03-30T09:06:14.846112Z"
    }
   },
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Baget.\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This course is produced by Baget.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "a7d68e49-3c24-4cf2-8a9d-9a0a956eac1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:21.850027Z",
     "start_time": "2025-03-30T09:06:20.418464Z"
    }
   },
   "source": [
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "0c2ff059-dae9-4d9a-a0a4-f440644fd0a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:23.579205Z",
     "start_time": "2025-03-30T09:06:21.856752Z"
    }
   },
   "source": [
    "filler = pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
    "result = filler(\"...\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "ename": "PipelineException",
     "evalue": "No mask_token ([MASK]) found on the input",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPipelineException\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m filler = pipeline(\u001B[33m\"\u001B[39m\u001B[33mfill-mask\u001B[39m\u001B[33m\"\u001B[39m, model=\u001B[33m\"\u001B[39m\u001B[33mbert-base-cased\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m result = \u001B[43mfiller\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m...\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/transformers/pipelines/fill_mask.py:270\u001B[39m, in \u001B[36mFillMaskPipeline.__call__\u001B[39m\u001B[34m(self, inputs, **kwargs)\u001B[39m\n\u001B[32m    248\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, **kwargs):\n\u001B[32m    249\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    250\u001B[39m \u001B[33;03m    Fill the masked token in the text(s) given as inputs.\u001B[39;00m\n\u001B[32m    251\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    268\u001B[39m \u001B[33;03m        - **token_str** (`str`) -- The predicted token (to replace the masked one).\u001B[39;00m\n\u001B[32m    269\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m270\u001B[39m     outputs = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    271\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(inputs) == \u001B[32m1\u001B[39m:\n\u001B[32m    272\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m outputs[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1371\u001B[39m, in \u001B[36mPipeline.__call__\u001B[39m\u001B[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[39m\n\u001B[32m   1363\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[32m   1364\u001B[39m         \u001B[38;5;28miter\u001B[39m(\n\u001B[32m   1365\u001B[39m             \u001B[38;5;28mself\u001B[39m.get_iterator(\n\u001B[32m   (...)\u001B[39m\u001B[32m   1368\u001B[39m         )\n\u001B[32m   1369\u001B[39m     )\n\u001B[32m   1370\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1371\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1377\u001B[39m, in \u001B[36mPipeline.run_single\u001B[39m\u001B[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[39m\n\u001B[32m   1376\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[32m-> \u001B[39m\u001B[32m1377\u001B[39m     model_inputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpreprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1378\u001B[39m     model_outputs = \u001B[38;5;28mself\u001B[39m.forward(model_inputs, **forward_params)\n\u001B[32m   1379\u001B[39m     outputs = \u001B[38;5;28mself\u001B[39m.postprocess(model_outputs, **postprocess_params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/transformers/pipelines/fill_mask.py:123\u001B[39m, in \u001B[36mFillMaskPipeline.preprocess\u001B[39m\u001B[34m(self, inputs, return_tensors, tokenizer_kwargs, **preprocess_parameters)\u001B[39m\n\u001B[32m    120\u001B[39m     tokenizer_kwargs = {}\n\u001B[32m    122\u001B[39m model_inputs = \u001B[38;5;28mself\u001B[39m.tokenizer(inputs, return_tensors=return_tensors, **tokenizer_kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m123\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mensure_exactly_one_mask_token\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m model_inputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/transformers/pipelines/fill_mask.py:112\u001B[39m, in \u001B[36mFillMaskPipeline.ensure_exactly_one_mask_token\u001B[39m\u001B[34m(self, model_inputs)\u001B[39m\n\u001B[32m    110\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    111\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m input_ids \u001B[38;5;129;01min\u001B[39;00m model_inputs[\u001B[33m\"\u001B[39m\u001B[33minput_ids\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m112\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_ensure_exactly_one_mask_token\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/transformers/pipelines/fill_mask.py:100\u001B[39m, in \u001B[36mFillMaskPipeline._ensure_exactly_one_mask_token\u001B[39m\u001B[34m(self, input_ids)\u001B[39m\n\u001B[32m     98\u001B[39m numel = np.prod(masked_index.shape)\n\u001B[32m     99\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m numel < \u001B[32m1\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m100\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m PipelineException(\n\u001B[32m    101\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mfill-mask\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    102\u001B[39m         \u001B[38;5;28mself\u001B[39m.model.base_model_prefix,\n\u001B[32m    103\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNo mask_token (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.tokenizer.mask_token\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) found on the input\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    104\u001B[39m     )\n",
      "\u001B[31mPipelineException\u001B[39m: No mask_token ([MASK]) found on the input"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "0af6ca8a-e6e1-4f67-9405-0f1cfb65db01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:35.698841Z",
     "start_time": "2025-03-30T09:06:33.869226Z"
    }
   },
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "3f632633-7cc0-4fc8-9314-7db06950c123",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:38.811080Z",
     "start_time": "2025-03-30T09:06:38.808403Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "2e2bd588-af67-462a-869d-82de2ce2000f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:40.562771Z",
     "start_time": "2025-03-30T09:06:40.367366Z"
    }
   },
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "205fcfdd-6b01-49e0-bacb-6ae17813d5cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:41.242269Z",
     "start_time": "2025-03-30T09:06:41.238215Z"
    }
   },
   "source": [
    "tokenizer(['Hi wooow aaaa', 'nkjsnfgkjnfv fvnkjsdfnvdfs'], padding=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7632, 15854, 5004, 13360, 2050, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 25930, 22578, 2078, 2546, 2290, 2243, 22895, 2546, 2615, 1042, 16022, 2243, 22578, 20952, 2078, 16872, 10343, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "26643d48-0a81-448d-a384-cd4ede77c1a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:42.729977Z",
     "start_time": "2025-03-30T09:06:42.723381Z"
    }
   },
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=False, return_tensors=\"pt\")\n",
    "print(inputs)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "13ce7207-a89e-403f-9f9e-4cb5224f09dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:43.839125Z",
     "start_time": "2025-03-30T09:06:43.835052Z"
    }
   },
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "b4b25f2b-c5b6-469e-b6b0-8aa241ab89d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:45.001659Z",
     "start_time": "2025-03-30T09:06:44.766847Z"
    }
   },
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "04b9c453-0256-49a3-8d0c-8cedec14fec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:46.199943Z",
     "start_time": "2025-03-30T09:06:45.883522Z"
    }
   },
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "ab6896d6-2e99-4f30-938e-2206edee0a86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:46.891981Z",
     "start_time": "2025-03-30T09:06:46.684073Z"
    }
   },
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "93e74810-74bd-41e5-9707-b45a0804b964",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:47.915072Z",
     "start_time": "2025-03-30T09:06:47.911836Z"
    }
   },
   "source": [
    "print(outputs)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "65f108af-90bc-4550-ae0e-e06721e34f11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:48.715301Z",
     "start_time": "2025-03-30T09:06:48.711142Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "dabc59ae-0993-4344-b53c-2207bce97c11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:06:49.668778Z",
     "start_time": "2025-03-30T09:06:49.664455Z"
    }
   },
   "source": [
    "model.config.id2label\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "3491b402-0b39-45fe-b27b-d89271555e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:35:36.807309Z",
     "start_time": "2025-03-30T09:35:35.450956Z"
    }
   },
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:35:43.396278Z",
     "start_time": "2025-03-30T09:35:43.393057Z"
    }
   },
   "cell_type": "code",
   "source": "print(config)",
   "id": "620260f3f9fe9a95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.50.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:45:42.119733Z",
     "start_time": "2025-03-30T09:45:41.671809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ],
   "id": "2bad34689e9d2c24",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:47:54.899167Z",
     "start_time": "2025-03-30T09:47:53.597268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ],
   "id": "ff3af58e73e692ca",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:48:03.628546Z",
     "start_time": "2025-03-30T09:48:03.619150Z"
    }
   },
   "cell_type": "code",
   "source": "output",
   "id": "70e88d6acd51d9a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.6023,  0.1092,  0.1417,  ..., -0.4177,  0.6059,  0.1764],\n",
       "         [ 0.5119, -0.4770,  0.5508,  ..., -0.2814,  0.3793,  0.1156],\n",
       "         [ 0.0995,  0.0867,  0.0869,  ...,  0.4789, -0.3236,  0.3122],\n",
       "         ...,\n",
       "         [ 0.8081, -0.7380,  0.2001,  ...,  0.7405, -0.7998,  0.6449],\n",
       "         [ 0.3305, -0.1958,  0.3148,  ..., -0.0525,  0.5358,  0.1987],\n",
       "         [ 0.5655, -0.2176, -0.4720,  ..., -0.3554,  0.6141, -0.2476]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.2835e-01,  6.0906e-01,  9.9998e-01, -9.9831e-01,  9.8739e-01,\n",
       "          9.2930e-01,  9.9664e-01, -9.8909e-01, -9.9124e-01, -7.8317e-01,\n",
       "          9.9476e-01,  9.9961e-01, -9.9850e-01, -9.9997e-01,  8.0649e-01,\n",
       "         -9.9098e-01,  9.9545e-01, -6.9968e-01, -9.9999e-01, -2.6556e-01,\n",
       "         -4.1867e-01, -9.9998e-01,  3.1562e-01,  9.7638e-01,  9.9257e-01,\n",
       "          1.4021e-01,  9.9543e-01,  9.9999e-01,  9.4214e-01, -2.0248e-01,\n",
       "          3.5963e-01, -9.9665e-01,  8.5152e-01, -9.9975e-01,  2.8570e-01,\n",
       "          7.8621e-03,  8.4606e-01, -3.7096e-01,  7.4315e-01, -9.5596e-01,\n",
       "         -7.8807e-01, -4.7754e-01,  6.0904e-01, -6.4658e-01,  9.6723e-01,\n",
       "          2.2952e-01,  2.4267e-01,  1.1464e-01, -1.1224e-01,  9.9997e-01,\n",
       "         -9.9227e-01,  9.9995e-01, -9.9455e-01,  9.9951e-01,  9.9903e-01,\n",
       "          4.6474e-01,  9.9825e-01,  1.8630e-01, -9.9897e-01,  4.2824e-01,\n",
       "          9.7877e-01,  9.9492e-02,  9.7601e-01, -2.5574e-01, -1.0145e-01,\n",
       "         -5.6067e-01, -8.6941e-01,  2.3935e-01, -6.2682e-01,  5.1608e-01,\n",
       "          4.8575e-01,  4.8747e-01,  9.9565e-01, -9.5639e-01, -1.1690e-01,\n",
       "         -9.4263e-01,  1.5285e-01, -9.9998e-01,  9.8651e-01,  9.9999e-01,\n",
       "          6.5605e-01, -9.9991e-01,  9.9870e-01, -4.0475e-01, -7.0561e-01,\n",
       "          2.7477e-01, -9.9917e-01, -9.9987e-01,  1.8895e-01, -6.6828e-01,\n",
       "          8.3371e-01, -9.9579e-01,  5.4993e-01, -9.1550e-01,  1.0000e+00,\n",
       "         -9.6308e-01, -2.8974e-01,  4.9713e-01,  9.5074e-01, -3.1329e-01,\n",
       "         -8.5073e-01,  9.2660e-01,  9.9924e-01, -9.9509e-01,  9.9877e-01,\n",
       "          5.7332e-01, -9.5204e-01, -8.4878e-01,  5.3101e-01,  2.2957e-01,\n",
       "          9.9633e-01, -9.9581e-01, -8.4645e-01,  2.3134e-01,  9.7124e-01,\n",
       "         -7.6645e-01,  9.9707e-01,  7.0794e-01, -4.3663e-01,  1.0000e+00,\n",
       "         -2.7179e-01,  9.8517e-01,  9.9961e-01,  9.0877e-01, -8.1168e-01,\n",
       "         -3.3681e-01, -4.8113e-01,  8.1254e-01, -3.3984e-01, -5.0071e-01,\n",
       "          8.5569e-01, -9.9618e-01, -9.9880e-01,  9.9988e-01, -3.4080e-01,\n",
       "          9.9999e-01, -9.9978e-01,  9.9567e-01, -9.9999e-01, -5.9676e-01,\n",
       "         -8.0271e-01, -1.6363e-02, -9.9063e-01,  2.9677e-01,  9.9670e-01,\n",
       "          1.9180e-01, -9.6363e-01, -6.3803e-01,  4.5937e-01, -8.5751e-01,\n",
       "          6.9634e-01,  8.6684e-01, -9.8712e-01,  9.9985e-01,  9.9661e-01,\n",
       "          9.6512e-01,  9.9064e-01,  3.0494e-01, -9.6835e-01,  8.8074e-01,\n",
       "          9.9611e-01, -9.9987e-01,  5.6770e-01, -9.9339e-01,  9.9983e-01,\n",
       "          9.9128e-01,  6.8117e-01, -9.9564e-01,  9.9998e-01, -4.5610e-01,\n",
       "          2.2760e-01, -1.9633e-01, -4.2445e-01, -9.9918e-01,  6.1411e-01,\n",
       "          5.4706e-01,  8.1350e-01,  9.9991e-01, -9.9806e-01,  9.9992e-01,\n",
       "          9.9405e-01, -2.4961e-01,  8.3604e-01,  9.9859e-01, -9.9852e-01,\n",
       "         -9.9445e-01, -9.9634e-01,  3.6493e-01,  5.9082e-01,  4.1785e-01,\n",
       "          4.1630e-01,  9.7932e-01,  9.9917e-01,  8.1510e-01, -9.9967e-01,\n",
       "         -4.6312e-01,  9.9122e-01, -2.8691e-01,  1.0000e+00,  2.9178e-01,\n",
       "         -9.9996e-01, -8.2507e-01,  9.6417e-01,  9.9644e-01, -4.4478e-01,\n",
       "          9.9324e-01, -5.7100e-01, -3.4784e-04,  9.8806e-01, -9.9977e-01,\n",
       "          9.9852e-01, -2.3978e-01,  8.4522e-01,  9.3744e-01,  9.9795e-01,\n",
       "         -8.0662e-01, -2.2260e-01,  3.8672e-01, -6.8129e-01,  9.9997e-01,\n",
       "         -9.9990e-01, -3.2183e-01,  5.9251e-01, -9.9829e-01, -9.9941e-01,\n",
       "          9.9586e-01, -1.0074e-01, -7.3508e-01, -2.5186e-01,  1.9904e-01,\n",
       "          3.7265e-01,  9.1428e-01,  9.9669e-01, -5.9080e-01, -2.5383e-01,\n",
       "         -9.9996e-01, -9.9760e-01, -8.7478e-01, -9.7038e-01,  2.0462e-01,\n",
       "          8.0368e-01, -5.2217e-01, -9.6130e-01, -9.9889e-01,  9.9059e-01,\n",
       "          6.4181e-01, -9.2359e-01, -5.0590e-01, -5.2063e-01, -9.9898e-01,\n",
       "          3.8437e-01, -8.1184e-01, -9.9977e-01,  9.9992e-01, -8.2068e-01,\n",
       "          9.9730e-01,  9.9512e-01, -9.9856e-01,  7.3626e-01, -9.9888e-01,\n",
       "         -7.3232e-03, -9.9984e-01,  1.2394e-01,  3.3718e-01, -7.3533e-01,\n",
       "         -8.3842e-02,  9.9802e-01, -9.8660e-01, -8.3092e-01,  8.1347e-01,\n",
       "         -9.9999e-01,  9.7834e-01, -3.6798e-01,  9.9975e-01,  7.8507e-01,\n",
       "         -7.4606e-02,  9.9465e-01,  9.3201e-01, -9.9514e-01, -9.9995e-01,\n",
       "          8.7159e-01,  9.9806e-01, -9.9809e-01, -3.5480e-01,  9.9999e-01,\n",
       "         -9.9877e-01, -8.3070e-01, -9.7494e-01, -9.9894e-01, -9.9993e-01,\n",
       "          1.5644e-01, -7.8414e-01,  6.0187e-02,  9.9392e-01,  2.4572e-01,\n",
       "          1.6016e-01,  9.9910e-01,  9.9941e-01,  2.2788e-01,  8.4308e-03,\n",
       "          1.5305e-01, -9.9167e-01, -9.9933e-01,  6.4604e-01,  3.7302e-01,\n",
       "         -9.9999e-01,  9.9997e-01, -9.9826e-01,  9.9987e-01,  9.6995e-01,\n",
       "         -9.9343e-01,  8.8091e-01,  1.4430e-02, -9.6750e-01,  1.0024e-01,\n",
       "          9.9998e-01,  9.9338e-01, -1.5025e-01,  3.1323e-01,  9.1669e-01,\n",
       "         -2.1946e-01,  4.9523e-01, -7.6086e-01, -5.7688e-01,  2.3562e-01,\n",
       "         -9.6419e-01,  9.9618e-01,  6.4859e-01, -9.9733e-01,  9.9700e-01,\n",
       "          6.5070e-02,  8.3284e-01, -7.4301e-01,  9.1747e-01,  9.9662e-01,\n",
       "         -2.7177e-01, -3.7245e-01, -4.3214e-02, -9.3129e-01, -9.7803e-01,\n",
       "          2.7836e-01, -9.9750e-01, -3.2621e-01,  9.6046e-01,  9.9694e-01,\n",
       "         -9.9700e-01,  9.9945e-01, -3.0036e-01,  8.7511e-01, -9.9853e-01,\n",
       "          1.0000e+00, -9.9977e-01,  2.8375e-01,  5.8693e-01, -9.1986e-01,\n",
       "         -5.8613e-01,  9.9782e-01,  9.8489e-01,  9.7992e-01, -8.6434e-01,\n",
       "         -4.6918e-01,  9.2471e-01,  9.9000e-01, -9.8676e-01,  5.5841e-02,\n",
       "         -9.9959e-01, -6.3961e-01,  9.9900e-01,  9.9601e-01, -1.8138e-01,\n",
       "         -5.7472e-01, -9.9791e-01,  9.8469e-01, -8.4956e-01, -7.2607e-01,\n",
       "         -1.8493e-01, -8.4716e-01,  5.6414e-01,  9.9854e-01, -2.8356e-01,\n",
       "          5.9711e-01,  2.0103e-01, -9.9755e-01,  8.3863e-01,  6.8787e-01,\n",
       "          9.9996e-01, -9.9299e-01,  3.1409e-01,  9.9636e-01, -3.3831e-01,\n",
       "         -6.6649e-01,  7.4109e-01,  9.9907e-01, -9.8940e-01, -3.6638e-01,\n",
       "         -9.9990e-01,  4.8964e-02, -8.5911e-01,  1.4593e-01, -2.9822e-01,\n",
       "          2.2590e-01, -8.4246e-01,  9.6926e-01,  1.0953e-01,  8.4557e-01,\n",
       "         -5.9906e-03,  9.8803e-01, -2.0995e-02, -1.0543e-01, -4.6601e-01,\n",
       "         -4.2787e-02,  6.1662e-01,  1.2547e-01,  9.9180e-01, -9.9351e-01,\n",
       "          9.9996e-01, -6.7802e-01, -9.9999e-01, -9.9770e-01, -5.7405e-01,\n",
       "         -9.9994e-01,  6.3853e-01, -9.9951e-01,  9.9676e-01,  9.6816e-01,\n",
       "         -9.9848e-01, -9.9946e-01, -9.9982e-01, -9.9976e-01,  6.1510e-01,\n",
       "          6.4182e-01, -2.1805e-01,  9.2796e-02,  9.0330e-01,  1.4380e-01,\n",
       "          1.2573e-01, -1.0409e-01, -9.7871e-01, -4.2303e-01, -9.9905e-01,\n",
       "          7.1276e-01, -9.9999e-01, -6.9909e-01,  9.9814e-01, -9.9562e-01,\n",
       "         -9.4415e-01, -9.6432e-01, -8.9964e-01, -9.2392e-01,  5.7695e-01,\n",
       "          9.9405e-01, -1.8583e-01, -5.1823e-01, -9.9992e-01,  9.9637e-01,\n",
       "         -8.6311e-01,  1.3339e-01, -8.4187e-01, -9.9070e-01,  9.9994e-01,\n",
       "          8.9447e-01, -1.2484e-01, -2.5460e-01, -9.9981e-01,  9.9253e-01,\n",
       "         -9.3657e-01, -9.1491e-01, -9.9574e-01,  3.4290e-01, -9.7082e-01,\n",
       "         -9.9998e-01,  1.6956e-01,  9.9788e-01,  9.9939e-01,  9.8977e-01,\n",
       "          1.9435e-01, -4.6757e-01, -9.7244e-01,  3.6547e-01, -9.9999e-01,\n",
       "          8.2877e-01,  8.5028e-01, -9.9369e-01, -5.5643e-01,  9.9792e-01,\n",
       "          9.9398e-01, -9.6296e-01, -9.8676e-01,  9.4785e-01,  7.4835e-01,\n",
       "          9.7754e-01, -3.5312e-01, -4.9721e-01,  4.5349e-01, -1.5718e-01,\n",
       "         -9.9632e-01, -9.8159e-01,  9.9876e-01, -9.9932e-01,  9.8976e-01,\n",
       "          9.9815e-01,  9.9910e-01,  2.7285e-01, -1.3299e-01, -9.9287e-01,\n",
       "         -9.9943e-01, -6.8430e-01,  3.2410e-01, -9.9999e-01,  9.9998e-01,\n",
       "         -1.0000e+00,  5.1310e-01, -6.4679e-01,  9.0606e-01,  9.9615e-01,\n",
       "         -3.3432e-01, -9.9998e-01, -9.9997e-01,  8.6537e-01, -3.1250e-01,\n",
       "          9.9661e-01,  2.3914e-01,  3.9441e-01, -5.5191e-01, -2.8731e-02,\n",
       "          9.9953e-01, -9.4059e-01, -6.8058e-01, -9.9860e-01,  9.9991e-01,\n",
       "          7.7975e-01, -9.9972e-01,  9.9650e-01, -9.9993e-01,  8.6067e-01,\n",
       "          9.9085e-01,  9.5399e-01,  9.9253e-01, -9.9939e-01,  1.0000e+00,\n",
       "         -9.9996e-01,  9.9958e-01, -9.9999e-01, -9.9914e-01,  9.9996e-01,\n",
       "         -9.9680e-01, -5.2242e-01, -9.9995e-01, -9.9846e-01,  7.6624e-01,\n",
       "          3.0813e-01, -6.0101e-01,  9.9681e-01, -9.9996e-01, -9.9965e-01,\n",
       "          5.0632e-01, -9.5096e-01, -7.7078e-01,  9.9660e-01, -5.1192e-01,\n",
       "          9.9864e-01, -1.3044e-02,  9.8215e-01,  1.4195e-01,  9.9789e-01,\n",
       "          9.9982e-01, -6.5892e-01, -4.9805e-01, -9.9783e-01,  9.9263e-01,\n",
       "         -6.6011e-01,  4.8956e-01,  9.8212e-01, -1.5472e-01, -4.5557e-01,\n",
       "          5.7543e-01, -9.9919e-01,  5.2152e-01, -8.3201e-01,  8.9039e-01,\n",
       "          9.4857e-01,  9.1789e-01,  1.1048e-02, -3.8681e-01, -1.1080e-01,\n",
       "         -9.9808e-01,  7.4150e-01, -9.9987e-01,  9.9106e-01, -9.6465e-01,\n",
       "          2.9540e-01, -5.3421e-01,  5.4674e-01, -9.7558e-01,  9.9991e-01,\n",
       "          9.9965e-01, -9.9994e-01,  2.2361e-01,  9.9665e-01, -6.4159e-01,\n",
       "          9.9177e-01, -9.9753e-01, -6.9763e-02,  9.6400e-01, -8.7434e-01,\n",
       "          9.9381e-01,  1.8743e-01, -2.3633e-01,  9.8364e-01, -9.9887e-01,\n",
       "         -9.0222e-01, -7.9110e-01,  3.5238e-01,  3.4325e-01, -9.8982e-01,\n",
       "          2.8097e-01,  9.8251e-01,  7.6614e-02, -9.9993e-01,  9.8067e-01,\n",
       "         -9.9986e-01, -3.6354e-01,  9.9292e-01,  1.4346e-01,  9.9998e-01,\n",
       "         -7.4159e-01,  3.1454e-02,  1.0397e-01, -9.9994e-01, -9.9913e-01,\n",
       "          2.1931e-01, -2.7733e-01, -9.5555e-01,  9.9938e-01, -5.0606e-02,\n",
       "          8.3410e-01, -9.9998e-01,  4.2417e-01,  9.9698e-01,  3.9198e-01,\n",
       "          9.1816e-01, -8.0208e-01, -9.7382e-01, -9.5518e-01, -6.6141e-01,\n",
       "          1.5180e-01,  9.0137e-01, -9.9532e-01, -9.1342e-01, -7.1074e-01,\n",
       "          9.9999e-01, -9.9956e-01, -9.7606e-01, -9.9647e-01,  4.1117e-01,\n",
       "          8.8649e-01,  6.1946e-01,  1.1032e-01, -8.1372e-01,  9.3144e-01,\n",
       "         -9.1679e-01,  9.9910e-01, -9.9854e-01, -9.9933e-01,  9.9996e-01,\n",
       "          6.8804e-01, -9.9306e-01, -1.1649e-01, -4.1921e-01,  7.9798e-02,\n",
       "         -6.4768e-03,  7.7816e-01, -9.5855e-01, -2.6136e-01, -9.9975e-01,\n",
       "          8.5560e-01, -8.4889e-01, -9.9709e-01, -6.7251e-01, -4.8243e-01,\n",
       "         -9.9988e-01,  9.9809e-01,  9.9115e-01,  9.9999e-01, -9.9996e-01,\n",
       "          8.7347e-01,  2.2774e-01,  9.9976e-01,  4.1655e-02, -7.7174e-01,\n",
       "          9.2765e-01,  9.9992e-01, -7.4625e-01,  7.7272e-01,  1.8972e-02,\n",
       "         -2.1511e-01,  4.3176e-02, -7.1345e-01,  9.9652e-01, -9.4827e-01,\n",
       "          3.5106e-01, -9.9568e-01, -9.9998e-01,  9.9999e-01, -1.2416e-01,\n",
       "          9.9711e-01,  3.3779e-01,  8.0375e-01, -8.9710e-01,  9.8400e-01,\n",
       "         -9.8929e-01, -9.3701e-01, -1.0000e+00,  2.1380e-01, -9.9995e-01,\n",
       "         -9.9693e-01,  2.9115e-01,  9.9771e-01, -9.9989e-01, -9.9722e-01,\n",
       "         -3.7007e-01, -1.0000e+00,  9.1409e-01, -9.9338e-01, -7.9647e-01,\n",
       "         -9.9685e-01,  9.9875e-01, -5.3951e-01, -4.7717e-01,  9.9424e-01,\n",
       "         -9.9099e-01,  9.3857e-01,  9.7499e-01,  7.2225e-01,  2.9207e-01,\n",
       "          3.1667e-01, -6.4539e-01, -9.9575e-01, -9.5048e-01, -9.8199e-01,\n",
       "          9.2237e-01, -9.9711e-01, -9.4075e-01,  9.9913e-01,  9.9735e-01,\n",
       "         -9.9988e-01, -9.9907e-01,  9.9719e-01, -2.8665e-01,  9.9731e-01,\n",
       "         -6.8889e-01, -9.9997e-01, -9.9998e-01,  1.8779e-01, -2.7697e-01,\n",
       "          9.9849e-01, -4.6101e-01,  9.9961e-01,  8.1148e-01, -8.5870e-02,\n",
       "          4.4598e-01, -3.9436e-01, -1.2326e-01, -2.3773e-01, -2.7982e-01,\n",
       "          9.9999e-01, -4.9552e-01,  9.9713e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T09:57:54.781245Z",
     "start_time": "2025-03-30T09:57:54.453670Z"
    }
   },
   "cell_type": "code",
   "source": "model.save_pretrained(\"my_folder\")",
   "id": "10d3dd44981c9401",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T10:09:01.031448Z",
     "start_time": "2025-03-30T10:09:00.802923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained(\"/Users/perchik/PycharmProjects/Learning_LLMs/my_folder\")\n"
   ],
   "id": "4af8e22ec61c97df",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T10:09:03.326544Z",
     "start_time": "2025-03-30T10:09:03.245531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "output"
   ],
   "id": "413783bb069b1568",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.6023,  0.1092,  0.1417,  ..., -0.4177,  0.6059,  0.1764],\n",
       "         [ 0.5119, -0.4770,  0.5508,  ..., -0.2814,  0.3793,  0.1156],\n",
       "         [ 0.0995,  0.0867,  0.0869,  ...,  0.4789, -0.3236,  0.3122],\n",
       "         ...,\n",
       "         [ 0.8081, -0.7380,  0.2001,  ...,  0.7405, -0.7998,  0.6449],\n",
       "         [ 0.3305, -0.1958,  0.3148,  ..., -0.0525,  0.5358,  0.1987],\n",
       "         [ 0.5655, -0.2176, -0.4720,  ..., -0.3554,  0.6141, -0.2476]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.2835e-01,  6.0906e-01,  9.9998e-01, -9.9831e-01,  9.8739e-01,\n",
       "          9.2930e-01,  9.9664e-01, -9.8909e-01, -9.9124e-01, -7.8317e-01,\n",
       "          9.9476e-01,  9.9961e-01, -9.9850e-01, -9.9997e-01,  8.0649e-01,\n",
       "         -9.9098e-01,  9.9545e-01, -6.9968e-01, -9.9999e-01, -2.6556e-01,\n",
       "         -4.1867e-01, -9.9998e-01,  3.1562e-01,  9.7638e-01,  9.9257e-01,\n",
       "          1.4021e-01,  9.9543e-01,  9.9999e-01,  9.4214e-01, -2.0248e-01,\n",
       "          3.5963e-01, -9.9665e-01,  8.5152e-01, -9.9975e-01,  2.8570e-01,\n",
       "          7.8622e-03,  8.4606e-01, -3.7096e-01,  7.4315e-01, -9.5596e-01,\n",
       "         -7.8807e-01, -4.7754e-01,  6.0904e-01, -6.4658e-01,  9.6723e-01,\n",
       "          2.2952e-01,  2.4267e-01,  1.1464e-01, -1.1224e-01,  9.9997e-01,\n",
       "         -9.9227e-01,  9.9995e-01, -9.9455e-01,  9.9951e-01,  9.9903e-01,\n",
       "          4.6474e-01,  9.9825e-01,  1.8630e-01, -9.9897e-01,  4.2824e-01,\n",
       "          9.7877e-01,  9.9492e-02,  9.7601e-01, -2.5574e-01, -1.0145e-01,\n",
       "         -5.6067e-01, -8.6941e-01,  2.3935e-01, -6.2682e-01,  5.1608e-01,\n",
       "          4.8575e-01,  4.8747e-01,  9.9565e-01, -9.5639e-01, -1.1690e-01,\n",
       "         -9.4263e-01,  1.5285e-01, -9.9998e-01,  9.8651e-01,  9.9999e-01,\n",
       "          6.5605e-01, -9.9991e-01,  9.9870e-01, -4.0475e-01, -7.0561e-01,\n",
       "          2.7477e-01, -9.9917e-01, -9.9987e-01,  1.8895e-01, -6.6828e-01,\n",
       "          8.3371e-01, -9.9579e-01,  5.4993e-01, -9.1550e-01,  1.0000e+00,\n",
       "         -9.6308e-01, -2.8974e-01,  4.9713e-01,  9.5074e-01, -3.1329e-01,\n",
       "         -8.5073e-01,  9.2660e-01,  9.9924e-01, -9.9509e-01,  9.9877e-01,\n",
       "          5.7332e-01, -9.5204e-01, -8.4878e-01,  5.3101e-01,  2.2957e-01,\n",
       "          9.9633e-01, -9.9581e-01, -8.4645e-01,  2.3134e-01,  9.7124e-01,\n",
       "         -7.6645e-01,  9.9707e-01,  7.0794e-01, -4.3663e-01,  1.0000e+00,\n",
       "         -2.7179e-01,  9.8517e-01,  9.9961e-01,  9.0877e-01, -8.1168e-01,\n",
       "         -3.3681e-01, -4.8113e-01,  8.1254e-01, -3.3984e-01, -5.0071e-01,\n",
       "          8.5569e-01, -9.9618e-01, -9.9880e-01,  9.9988e-01, -3.4080e-01,\n",
       "          9.9999e-01, -9.9978e-01,  9.9567e-01, -9.9999e-01, -5.9676e-01,\n",
       "         -8.0271e-01, -1.6363e-02, -9.9063e-01,  2.9677e-01,  9.9670e-01,\n",
       "          1.9180e-01, -9.6363e-01, -6.3803e-01,  4.5937e-01, -8.5751e-01,\n",
       "          6.9634e-01,  8.6684e-01, -9.8712e-01,  9.9985e-01,  9.9661e-01,\n",
       "          9.6512e-01,  9.9064e-01,  3.0494e-01, -9.6835e-01,  8.8074e-01,\n",
       "          9.9611e-01, -9.9987e-01,  5.6770e-01, -9.9339e-01,  9.9983e-01,\n",
       "          9.9128e-01,  6.8117e-01, -9.9564e-01,  9.9998e-01, -4.5610e-01,\n",
       "          2.2760e-01, -1.9633e-01, -4.2445e-01, -9.9918e-01,  6.1411e-01,\n",
       "          5.4706e-01,  8.1350e-01,  9.9991e-01, -9.9806e-01,  9.9992e-01,\n",
       "          9.9405e-01, -2.4961e-01,  8.3604e-01,  9.9859e-01, -9.9852e-01,\n",
       "         -9.9445e-01, -9.9634e-01,  3.6493e-01,  5.9082e-01,  4.1785e-01,\n",
       "          4.1630e-01,  9.7932e-01,  9.9917e-01,  8.1510e-01, -9.9967e-01,\n",
       "         -4.6312e-01,  9.9122e-01, -2.8691e-01,  1.0000e+00,  2.9178e-01,\n",
       "         -9.9996e-01, -8.2507e-01,  9.6417e-01,  9.9644e-01, -4.4478e-01,\n",
       "          9.9324e-01, -5.7100e-01, -3.4781e-04,  9.8806e-01, -9.9977e-01,\n",
       "          9.9852e-01, -2.3978e-01,  8.4522e-01,  9.3744e-01,  9.9795e-01,\n",
       "         -8.0662e-01, -2.2260e-01,  3.8672e-01, -6.8129e-01,  9.9997e-01,\n",
       "         -9.9990e-01, -3.2183e-01,  5.9251e-01, -9.9829e-01, -9.9941e-01,\n",
       "          9.9586e-01, -1.0074e-01, -7.3508e-01, -2.5186e-01,  1.9904e-01,\n",
       "          3.7265e-01,  9.1428e-01,  9.9669e-01, -5.9080e-01, -2.5383e-01,\n",
       "         -9.9996e-01, -9.9760e-01, -8.7478e-01, -9.7038e-01,  2.0462e-01,\n",
       "          8.0368e-01, -5.2217e-01, -9.6130e-01, -9.9889e-01,  9.9059e-01,\n",
       "          6.4181e-01, -9.2359e-01, -5.0590e-01, -5.2063e-01, -9.9898e-01,\n",
       "          3.8437e-01, -8.1184e-01, -9.9977e-01,  9.9992e-01, -8.2068e-01,\n",
       "          9.9730e-01,  9.9512e-01, -9.9856e-01,  7.3626e-01, -9.9888e-01,\n",
       "         -7.3232e-03, -9.9984e-01,  1.2394e-01,  3.3718e-01, -7.3533e-01,\n",
       "         -8.3842e-02,  9.9802e-01, -9.8660e-01, -8.3092e-01,  8.1347e-01,\n",
       "         -9.9999e-01,  9.7834e-01, -3.6798e-01,  9.9975e-01,  7.8507e-01,\n",
       "         -7.4606e-02,  9.9465e-01,  9.3201e-01, -9.9514e-01, -9.9995e-01,\n",
       "          8.7159e-01,  9.9806e-01, -9.9809e-01, -3.5480e-01,  9.9999e-01,\n",
       "         -9.9877e-01, -8.3070e-01, -9.7494e-01, -9.9894e-01, -9.9993e-01,\n",
       "          1.5644e-01, -7.8414e-01,  6.0187e-02,  9.9392e-01,  2.4572e-01,\n",
       "          1.6016e-01,  9.9910e-01,  9.9941e-01,  2.2788e-01,  8.4307e-03,\n",
       "          1.5305e-01, -9.9167e-01, -9.9933e-01,  6.4604e-01,  3.7302e-01,\n",
       "         -9.9999e-01,  9.9997e-01, -9.9826e-01,  9.9987e-01,  9.6995e-01,\n",
       "         -9.9343e-01,  8.8091e-01,  1.4430e-02, -9.6750e-01,  1.0024e-01,\n",
       "          9.9998e-01,  9.9338e-01, -1.5025e-01,  3.1323e-01,  9.1669e-01,\n",
       "         -2.1946e-01,  4.9523e-01, -7.6086e-01, -5.7688e-01,  2.3562e-01,\n",
       "         -9.6419e-01,  9.9618e-01,  6.4859e-01, -9.9733e-01,  9.9700e-01,\n",
       "          6.5070e-02,  8.3284e-01, -7.4301e-01,  9.1747e-01,  9.9662e-01,\n",
       "         -2.7177e-01, -3.7245e-01, -4.3214e-02, -9.3129e-01, -9.7803e-01,\n",
       "          2.7836e-01, -9.9750e-01, -3.2621e-01,  9.6046e-01,  9.9694e-01,\n",
       "         -9.9700e-01,  9.9945e-01, -3.0036e-01,  8.7511e-01, -9.9853e-01,\n",
       "          1.0000e+00, -9.9977e-01,  2.8375e-01,  5.8693e-01, -9.1986e-01,\n",
       "         -5.8613e-01,  9.9782e-01,  9.8489e-01,  9.7992e-01, -8.6434e-01,\n",
       "         -4.6918e-01,  9.2471e-01,  9.9000e-01, -9.8676e-01,  5.5841e-02,\n",
       "         -9.9959e-01, -6.3961e-01,  9.9900e-01,  9.9601e-01, -1.8138e-01,\n",
       "         -5.7472e-01, -9.9791e-01,  9.8469e-01, -8.4956e-01, -7.2607e-01,\n",
       "         -1.8493e-01, -8.4716e-01,  5.6414e-01,  9.9854e-01, -2.8356e-01,\n",
       "          5.9711e-01,  2.0103e-01, -9.9755e-01,  8.3863e-01,  6.8787e-01,\n",
       "          9.9996e-01, -9.9299e-01,  3.1409e-01,  9.9636e-01, -3.3831e-01,\n",
       "         -6.6649e-01,  7.4109e-01,  9.9907e-01, -9.8940e-01, -3.6638e-01,\n",
       "         -9.9990e-01,  4.8965e-02, -8.5911e-01,  1.4593e-01, -2.9822e-01,\n",
       "          2.2590e-01, -8.4246e-01,  9.6926e-01,  1.0953e-01,  8.4557e-01,\n",
       "         -5.9906e-03,  9.8803e-01, -2.0995e-02, -1.0543e-01, -4.6601e-01,\n",
       "         -4.2787e-02,  6.1662e-01,  1.2547e-01,  9.9180e-01, -9.9351e-01,\n",
       "          9.9996e-01, -6.7802e-01, -9.9999e-01, -9.9770e-01, -5.7405e-01,\n",
       "         -9.9994e-01,  6.3853e-01, -9.9951e-01,  9.9676e-01,  9.6816e-01,\n",
       "         -9.9848e-01, -9.9946e-01, -9.9982e-01, -9.9976e-01,  6.1510e-01,\n",
       "          6.4182e-01, -2.1805e-01,  9.2797e-02,  9.0330e-01,  1.4380e-01,\n",
       "          1.2573e-01, -1.0409e-01, -9.7871e-01, -4.2303e-01, -9.9905e-01,\n",
       "          7.1276e-01, -9.9999e-01, -6.9909e-01,  9.9814e-01, -9.9562e-01,\n",
       "         -9.4415e-01, -9.6432e-01, -8.9964e-01, -9.2392e-01,  5.7695e-01,\n",
       "          9.9405e-01, -1.8583e-01, -5.1823e-01, -9.9992e-01,  9.9637e-01,\n",
       "         -8.6311e-01,  1.3339e-01, -8.4187e-01, -9.9070e-01,  9.9994e-01,\n",
       "          8.9447e-01, -1.2484e-01, -2.5460e-01, -9.9981e-01,  9.9253e-01,\n",
       "         -9.3657e-01, -9.1491e-01, -9.9574e-01,  3.4290e-01, -9.7082e-01,\n",
       "         -9.9998e-01,  1.6956e-01,  9.9788e-01,  9.9939e-01,  9.8977e-01,\n",
       "          1.9435e-01, -4.6757e-01, -9.7244e-01,  3.6547e-01, -9.9999e-01,\n",
       "          8.2877e-01,  8.5028e-01, -9.9369e-01, -5.5643e-01,  9.9792e-01,\n",
       "          9.9398e-01, -9.6296e-01, -9.8676e-01,  9.4785e-01,  7.4835e-01,\n",
       "          9.7754e-01, -3.5312e-01, -4.9721e-01,  4.5349e-01, -1.5718e-01,\n",
       "         -9.9632e-01, -9.8159e-01,  9.9876e-01, -9.9932e-01,  9.8976e-01,\n",
       "          9.9815e-01,  9.9910e-01,  2.7285e-01, -1.3299e-01, -9.9287e-01,\n",
       "         -9.9943e-01, -6.8430e-01,  3.2410e-01, -9.9999e-01,  9.9998e-01,\n",
       "         -1.0000e+00,  5.1310e-01, -6.4679e-01,  9.0606e-01,  9.9615e-01,\n",
       "         -3.3432e-01, -9.9998e-01, -9.9997e-01,  8.6537e-01, -3.1250e-01,\n",
       "          9.9661e-01,  2.3914e-01,  3.9441e-01, -5.5191e-01, -2.8731e-02,\n",
       "          9.9953e-01, -9.4059e-01, -6.8058e-01, -9.9860e-01,  9.9991e-01,\n",
       "          7.7975e-01, -9.9972e-01,  9.9650e-01, -9.9993e-01,  8.6067e-01,\n",
       "          9.9085e-01,  9.5399e-01,  9.9253e-01, -9.9939e-01,  1.0000e+00,\n",
       "         -9.9996e-01,  9.9958e-01, -9.9999e-01, -9.9914e-01,  9.9996e-01,\n",
       "         -9.9680e-01, -5.2242e-01, -9.9995e-01, -9.9846e-01,  7.6624e-01,\n",
       "          3.0813e-01, -6.0101e-01,  9.9681e-01, -9.9996e-01, -9.9965e-01,\n",
       "          5.0632e-01, -9.5096e-01, -7.7078e-01,  9.9660e-01, -5.1192e-01,\n",
       "          9.9864e-01, -1.3044e-02,  9.8215e-01,  1.4195e-01,  9.9789e-01,\n",
       "          9.9982e-01, -6.5892e-01, -4.9805e-01, -9.9783e-01,  9.9263e-01,\n",
       "         -6.6011e-01,  4.8956e-01,  9.8212e-01, -1.5472e-01, -4.5557e-01,\n",
       "          5.7543e-01, -9.9919e-01,  5.2152e-01, -8.3201e-01,  8.9039e-01,\n",
       "          9.4857e-01,  9.1789e-01,  1.1048e-02, -3.8681e-01, -1.1080e-01,\n",
       "         -9.9808e-01,  7.4150e-01, -9.9987e-01,  9.9106e-01, -9.6465e-01,\n",
       "          2.9540e-01, -5.3421e-01,  5.4674e-01, -9.7558e-01,  9.9991e-01,\n",
       "          9.9965e-01, -9.9994e-01,  2.2361e-01,  9.9665e-01, -6.4159e-01,\n",
       "          9.9177e-01, -9.9753e-01, -6.9763e-02,  9.6400e-01, -8.7434e-01,\n",
       "          9.9381e-01,  1.8743e-01, -2.3633e-01,  9.8364e-01, -9.9887e-01,\n",
       "         -9.0222e-01, -7.9110e-01,  3.5238e-01,  3.4325e-01, -9.8982e-01,\n",
       "          2.8097e-01,  9.8251e-01,  7.6614e-02, -9.9993e-01,  9.8067e-01,\n",
       "         -9.9986e-01, -3.6354e-01,  9.9292e-01,  1.4346e-01,  9.9998e-01,\n",
       "         -7.4159e-01,  3.1454e-02,  1.0397e-01, -9.9994e-01, -9.9913e-01,\n",
       "          2.1931e-01, -2.7733e-01, -9.5555e-01,  9.9938e-01, -5.0606e-02,\n",
       "          8.3410e-01, -9.9998e-01,  4.2417e-01,  9.9698e-01,  3.9198e-01,\n",
       "          9.1816e-01, -8.0208e-01, -9.7382e-01, -9.5518e-01, -6.6141e-01,\n",
       "          1.5180e-01,  9.0137e-01, -9.9532e-01, -9.1342e-01, -7.1074e-01,\n",
       "          9.9999e-01, -9.9956e-01, -9.7606e-01, -9.9647e-01,  4.1117e-01,\n",
       "          8.8649e-01,  6.1946e-01,  1.1032e-01, -8.1372e-01,  9.3144e-01,\n",
       "         -9.1679e-01,  9.9910e-01, -9.9854e-01, -9.9933e-01,  9.9996e-01,\n",
       "          6.8804e-01, -9.9306e-01, -1.1649e-01, -4.1921e-01,  7.9798e-02,\n",
       "         -6.4768e-03,  7.7816e-01, -9.5855e-01, -2.6136e-01, -9.9975e-01,\n",
       "          8.5560e-01, -8.4889e-01, -9.9709e-01, -6.7251e-01, -4.8243e-01,\n",
       "         -9.9988e-01,  9.9809e-01,  9.9115e-01,  9.9999e-01, -9.9996e-01,\n",
       "          8.7347e-01,  2.2774e-01,  9.9976e-01,  4.1655e-02, -7.7174e-01,\n",
       "          9.2765e-01,  9.9992e-01, -7.4625e-01,  7.7272e-01,  1.8972e-02,\n",
       "         -2.1511e-01,  4.3176e-02, -7.1345e-01,  9.9652e-01, -9.4827e-01,\n",
       "          3.5106e-01, -9.9568e-01, -9.9998e-01,  9.9999e-01, -1.2416e-01,\n",
       "          9.9711e-01,  3.3779e-01,  8.0375e-01, -8.9710e-01,  9.8400e-01,\n",
       "         -9.8929e-01, -9.3701e-01, -1.0000e+00,  2.1380e-01, -9.9995e-01,\n",
       "         -9.9693e-01,  2.9115e-01,  9.9771e-01, -9.9989e-01, -9.9722e-01,\n",
       "         -3.7007e-01, -1.0000e+00,  9.1409e-01, -9.9338e-01, -7.9647e-01,\n",
       "         -9.9685e-01,  9.9875e-01, -5.3951e-01, -4.7717e-01,  9.9424e-01,\n",
       "         -9.9099e-01,  9.3857e-01,  9.7499e-01,  7.2225e-01,  2.9207e-01,\n",
       "          3.1667e-01, -6.4539e-01, -9.9575e-01, -9.5048e-01, -9.8199e-01,\n",
       "          9.2237e-01, -9.9711e-01, -9.4075e-01,  9.9913e-01,  9.9735e-01,\n",
       "         -9.9988e-01, -9.9907e-01,  9.9719e-01, -2.8665e-01,  9.9731e-01,\n",
       "         -6.8889e-01, -9.9997e-01, -9.9998e-01,  1.8779e-01, -2.7697e-01,\n",
       "          9.9849e-01, -4.6101e-01,  9.9961e-01,  8.1148e-01, -8.5870e-02,\n",
       "          4.4598e-01, -3.9436e-01, -1.2326e-01, -2.3773e-01, -2.7982e-01,\n",
       "          9.9999e-01, -4.9552e-01,  9.9713e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5611b514ab22c879"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
